# -*- coding: utf-8 -*-
"""TrainVit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yNQWsJHm69kA2GBenobtCM3nevDmZj8O

NOTE: code may not run locally without modification.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import TrOCRProcessor
from transformers import VisionEncoderDecoderModel
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
from transformers import default_data_collator
from datasets import load_metric
from transformers import ViTImageProcessor, ViTForImageClassification
from PIL import Image
import requests
from transformers import TrainingArguments
from transformers import Trainer
import numpy as np
from datasets import load_metric


# Change folder paths to something local
file_path = "/content/drive/MyDrive/WPI/Handwriting MQP/Code and Data/ClassifierData/labels.json"
image_dir = "/content/drive/MyDrive/WPI/Handwriting MQP/Code and Data/ClassifierData/images"

processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')


# Gets images and labels from JSON file and splits them
def get_image_data(json_path):
    df = pd.read_json(json_path)
    #df = df.head(1000)
    train_df, test_df = train_test_split(df, test_size=0.2)
    # we reset the indices to start from zero
    train_df.reset_index(drop=True, inplace=True)
    test_df.reset_index(drop=True, inplace=True)
    return train_df, test_df

# Turns images into a Pytorch Dataset
class CustomDataset(Dataset):
    def __init__(self, image_dir, df, processor, max_target_length=1280):
        self.image_dir = image_dir
        self.df = df
        self.processor = processor
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # get file name + text
        file_name = self.df['path'][idx]
        text = self.df['label'][idx]
        # prepare image (i.e. resize + normalize)
        image = Image.open(self.image_dir + file_name).convert("RGB")
        pixel_values = self.processor(image, return_tensors="pt").pixel_values
        # add labels (input_ids) by encoding the text
        labels = self.processor.tokenizer(text,
                                          padding="max_length",
                                          max_length=self.max_target_length).input_ids
        # important: make sure that PAD tokens are ignored by the loss function
       #labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]

        encoding = {"pixel_values": pixel_values.squeeze(), "labels": torch.tensor(labels)}
        return encoding

# Creates training and evaluation datasets
def create_datasets(train_df, test_df):
    train = CustomDataset(image_dir=image_dir,
                          df=train_df,
                          processor=processor)
    eval = CustomDataset(image_dir=image_dir,
                         df=test_df,
                         processor=processor)
    print("Number of training examples:", len(train))
    print("Number of validation examples:", len(eval))
    return train, eval

# Trains models using seq2seq trainer and parameters from the huggingface website's recommendations
def train_model(train_dataset, eval_dataset):
    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    # make sure vocab size is set correctly
    model.config.vocab_size = model.config.decoder.vocab_size

    # set beam search parameters
    model.config.eos_token_id = processor.tokenizer.sep_token_id
    model.config.max_length = 64
    model.config.early_stopping = True
    model.config.no_repeat_ngram_size = 3
    model.config.length_penalty = 2.0
    model.config.num_beams = 4

    training_args = TrainingArguments(
        output_dir="./",
        per_device_train_batch_size=16,
        evaluation_strategy="steps",
        num_train_epochs=4,
        fp16=True,
        save_steps=100,
        eval_steps=100,
        logging_steps=10,
        learning_rate=2e-4,
        save_total_limit=2,
        remove_unused_columns=False,
        push_to_hub=False,
        #report_to='tensorboard',
        load_best_model_at_end=True,
    )

    # instantiate trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=collate_fn,
        compute_metrics=compute_metrics,
        train_dataset=prepared_ds["train"],
        eval_dataset=prepared_ds["validation"],
        tokenizer=processor,
    )
    trainer.train()
    trainer.save_model("./")
    # Change path depending on where to save model
    trainer.save_model("/content/drive/MyDrive/WPI/Handwriting MQP/Code and Data/ClassifierData/Models/Model1/")
    trainer.log_metrics("train", train_results.metrics)
    trainer.save_metrics("train", train_results.metrics)
    trainer.save_state()


metric = load_metric("accuracy")

# Function from hugging face recommended for metric computation
def compute_metrics(p):
    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)

# Function to batch pictures and labels
def collate_fn(batch):
    return {
        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),
        'labels': torch.tensor([x['labels'] for x in batch])
    }


if __name__ == '__main__':
    train_df, test_df = get_image_data(file_path)
    train_dataset, eval_dataset = create_datasets(train_df, test_df)
    train_model(train_dataset, eval_dataset)

